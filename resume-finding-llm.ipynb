{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10746203,"sourceType":"datasetVersion","datasetId":6664380}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install pdfplumber\n!pip install accelerate==0.26.0\n!pip install --upgrade transformers\n!pip install -U bitsandbytes\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:24:09.884980Z","iopub.execute_input":"2025-03-11T10:24:09.885359Z","iopub.status.idle":"2025-03-11T10:24:36.249694Z","shell.execute_reply.started":"2025-03-11T10:24:09.885328Z","shell.execute_reply":"2025-03-11T10:24:36.248576Z"}},"outputs":[{"name":"stdout","text":"Collecting pdfplumber\n  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\nDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\nCollecting accelerate==0.26.0\n  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (0.28.1)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (0.4.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->accelerate==0.26.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->accelerate==0.26.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->accelerate==0.26.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->accelerate==0.26.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->accelerate==0.26.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->accelerate==0.26.0) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.0) (4.67.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->accelerate==0.26.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->accelerate==0.26.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->accelerate==0.26.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->accelerate==0.26.0) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->accelerate==0.26.0) (2024.2.0)\nDownloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\nSuccessfully installed accelerate-0.26.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\nSuccessfully installed transformers-4.49.0\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport io\nimport pandas as pd\nfrom tqdm import tqdm \nimport pdfplumber\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nimport pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,BitsAndBytesConfig\ntorch.random.manual_seed(0)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3.5-mini-instruct\", device_map=\"cuda\", torch_dtype=\"auto\",quantization_config=BitsAndBytesConfig(load_in_4bit=True), trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\ndef extract_pdf_content(pdf_path):\n        with pdfplumber.open(pdf_path) as pdf:\n            text = \"\"\n            # Loop through each page in the PDF\n            for page in pdf.pages:\n                text += page.extract_text()  # Extract text from the page\n        return text\n\n\n\ndef phi_3(resume_text,job_description,model=model,tokenizer=tokenizer):\n    messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are an AI designed to analyze resumes and compare them with job descriptions. Your task is to determine how well a given resume matches a job description by calculating a match percentage, identifying key skills mentioned in the JD that are present or missing in the resume, and highlighting any gaps such as experience level mismatches. Provide a structured and detailed response, including both positive and negative aspects of the match.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"I will provide a resume text and a job description. Your task is to evaluate the match percentage, list the matched and missing key skills, highlight any mismatches (such as experience, education, or domain expertise), and provide a structured assessment explaining how well the resume aligns with the job description.\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Understood. I will analyze the resume against the job description, calculate the match percentage, identify key skills that match or are missing, and highlight mismatches such as experience level, required certifications, or specific industry/domain expertise. I will then provide a clear and structured assessment, including both strengths and weaknesses.\"\n    },\n    {\"role\": \"user\", \"content\": resume_text},  # Replace this with the actual resume text\n    {\"role\": \"user\", \"content\": job_description}  # Replace this with the actual job description\n    ]\n\n\n\n    generation_args = {\n    \"max_new_tokens\": 1500,\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.9,\n    \"use_cache\": False  # Disable caching to avoid DynamicCache issues\n}\n    \n    output = pipe(messages, **generation_args)\n    return output[0]['generated_text']\n\n    \ntotal_co=''\npdf_path=input('enter the pdf path :____')\npdf_m=extract_pdf_content(pdf_path)\nque = input('paste the job discription____ ')\ntotal_co=phi_3(resume_text=pdf_m,job_description=que)\nprint(total_co)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:35:59.835037Z","iopub.execute_input":"2025-03-11T10:35:59.835421Z","execution_failed":"2025-03-11T11:42:41.707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acff9954d10c4d04ba7dead8ff8f4189"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"enter the pdf path :____ /kaggle/input/my-resume/N.surya_narayanan_resume_MscDs_.pdf (1) (1).pdf\npaste the job discription____  Job Title: Data Scientist (Deep Learning & Generative AI) ğŸ“ Location: Chennai, Noida ğŸ“„ Job Type: Full-time ğŸ’¼ Experience: 2-3 years  About the Role: We are seeking a Data Scientist specializing in Deep Learning and Generative AI with 2-3 years of experience to join our AI-driven team. This role is ideal for someone passionate about Neural Networks, Transformers, Large Language Models (LLMs), and NLP. You will be responsible for building, fine-tuning, and deploying AI models using Hugging Face, TensorFlow, PyTorch, and OpenAI APIs.  Responsibilities: âœ”ï¸ Develop and fine-tune Deep Learning models for NLP, CV, and Generative AI tasks. âœ”ï¸ Implement and optimize Transformer-based models (e.g., GPT, BERT, T5, LLaMA, Mistral). âœ”ï¸ Work with Hugging Faceâ€™s Transformers, Tokenizers, and Datasets for model training and deployment. âœ”ï¸ Design LLM-powered applications using LangChain, RAG, and Prompt Engineering techniques. âœ”ï¸ Handle data preprocessing, augmentation, and embedding techniques (FAISS, ChromaDB, Pinecone, Weaviate). âœ”ï¸ Train and optimize Diffusion Models (Stable Diffusion, DALLÂ·E) and GANs for image generation tasks. âœ”ï¸ Work with cloud platforms (AWS, GCP, Azure) for scalable model training and deployment. âœ”ï¸ Deploy AI models using FastAPI, Flask, Docker, and Kubernetes in production environments. âœ”ï¸ Collaborate with cross-functional teams to build AI-powered applications and solutions. âœ”ï¸ Stay updated with cutting-edge advancements in AI, Deep Learning, and Generative AI.  Requirements: ğŸ“ Bachelorâ€™s/Masterâ€™s degree in Computer Science, AI, Data Science, or related fields. ğŸ’» 2-3 years of hands-on experience in Deep Learning, NLP, or Generative AI projects. ğŸ Strong Python skills (NumPy, Pandas, PyTorch, TensorFlow, Hugging Face Transformers). ğŸ§  Solid understanding of Neural Networks, Attention Mechanisms, and Transfer Learning. ğŸ” Experience with LLMs, RAG, and model fine-tuning using Hugging Face or OpenAI APIs. ğŸ“Š Proficiency in vector databases (FAISS, ChromaDB, Pinecone) for embedding search. â˜ï¸ Hands-on experience with Cloud AI services (AWS SageMaker, GCP Vertex AI, Azure AI). ğŸ› ï¸ Experience with MLOps tools (MLflow, Kubeflow, Docker, Kubernetes, CI/CD pipelines). ğŸ“ˆ Strong grasp of Deep Learning frameworks (PyTorch Lightning, TensorFlow/Keras). ğŸ’¡ Problem-solving mindset and ability to work on real-world AI challenges.  Nice to Have: âœ… Experience with Diffusion Models (Stable Diffusion, DALLÂ·E, Midjourney). âœ… Knowledge of Graph Neural Networks (GNNs) and Reinforcement Learning (RLHF). âœ… Exposure to AI Ethics, Bias Mitigation, and Explainable AI (XAI). âœ… Contributions to open-source AI projects or research publications.  ğŸš€ Join us and work on cutting-edge AI, Deep Learning, and Generative AI models!\n"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pdfplumber\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntorch.random.manual_seed(0)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3.5-mini-instruct\", \n    device_map=device, \n    torch_dtype=\"auto\",\n    # quantization_config=BitsAndBytesConfig(load_in_4bit=True), \n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n\ndef extract_pdf_content(pdf_path):\n    with pdfplumber.open(pdf_path) as pdf:\n        text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n    return text\n\ndef phi_3_stream(resume_text, job_description):\n    messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are an AI designed to analyze resumes and compare them with job descriptions. Your task is to determine how well a given resume matches a job description by calculating a match percentage, identifying key skills mentioned in the JD that are present or missing in the resume, and highlighting any gaps such as experience level mismatches. Provide a structured and detailed response, including both positive and negative aspects of the match.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"I will provide a resume text and a job description. Your task is to evaluate the match percentage, list the matched and missing key skills, highlight any mismatches (such as experience, education, or domain expertise), and provide a structured assessment explaining how well the resume aligns with the job description.\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Understood. I will analyze the resume against the job description, calculate the match percentage, identify key skills that match or are missing, and highlight mismatches such as experience level, required certifications, or specific industry/domain expertise. I will then provide a clear and structured assessment, including both strengths and weaknesses.\"\n    },\n    {\"role\": \"user\", \"content\": resume_text},  # Replace this with the actual resume text\n    {\"role\": \"user\", \"content\": job_description}  # Replace this with the actual job description\n    ]\n    \n    input_text = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n    \n    generation_args = {\n        \"max_new_tokens\": 1500,\n        \"do_sample\": True,\n        \"temperature\": 0.7,\n        \"top_k\": 50,\n        \"top_p\": 0.9\n    }\n    \n    print(\"\\nAI Response:\")\n    with torch.no_grad():\n        output_ids = model.generate(input_text, **generation_args)\n        for token_id in output_ids[:, input_text.shape[-1]:][0]:\n            print(tokenizer.decode([token_id], skip_special_tokens=True), end=\"\", flush=True)\n\npdf_path = input('Enter the PDF path: ')\npdf_text = extract_pdf_content(pdf_path)\njob_description = input('Paste the job description: ')\nphi_3_stream(resume_text=pdf_text, job_description=job_description)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:24:36.250930Z","iopub.execute_input":"2025-03-11T10:24:36.251163Z","iopub.status.idle":"2025-03-11T10:25:50.041259Z","shell.execute_reply.started":"2025-03-11T10:24:36.251141Z","shell.execute_reply":"2025-03-11T10:25:50.039952Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9879325789314056b46ca658b25c0853"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a792fdb3c414659b9dee06d8061b534"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95332f03b554a6095c25a8543b87ab7"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51db276137974bc588bfd378fc20dc95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf259c73f66c4449ae86d324ff6d090f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29dd65393eb848248778b37de3542fd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b181989246d4b6988385f0df570282f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d96e3445e8c44f4babf000ba1ce2e25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4d05ec54cd4d1d879a4f606d5eb18b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6480b0b2464e55b4c243e61f33f37f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8a421b9abd4322a04eea46df70a672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca5a71b2cab43c29989a42af64dbe3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88408fab19849459ce888cb9263b0c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eec32157f18e44efa6d63f703054ae64"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"Enter the PDF path:  /kaggle/input/my-resume/N.surya_narayanan_resume_MscDs_.pdf (1) (1).pdf\nPaste the job description:  Job Title: Data Scientist (Deep Learning & Generative AI) ğŸ“ Location: Chennai, Noida ğŸ“„ Job Type: Full-time ğŸ’¼ Experience: 2-3 years  About the Role: We are seeking a Data Scientist specializing in Deep Learning and Generative AI with 2-3 years of experience to join our AI-driven team. This role is ideal for someone passionate about Neural Networks, Transformers, Large Language Models (LLMs), and NLP. You will be responsible for building, fine-tuning, and deploying AI models using Hugging Face, TensorFlow, PyTorch, and OpenAI APIs.  Responsibilities: âœ”ï¸ Develop and fine-tune Deep Learning models for NLP, CV, and Generative AI tasks. âœ”ï¸ Implement and optimize Transformer-based models (e.g., GPT, BERT, T5, LLaMA, Mistral). âœ”ï¸ Work with Hugging Faceâ€™s Transformers, Tokenizers, and Datasets for model training and deployment. âœ”ï¸ Design LLM-powered applications using LangChain, RAG, and Prompt Engineering techniques. âœ”ï¸ Handle data preprocessing, augmentation, and embedding techniques (FAISS, ChromaDB, Pinecone, Weaviate). âœ”ï¸ Train and optimize Diffusion Models (Stable Diffusion, DALLÂ·E) and GANs for image generation tasks. âœ”ï¸ Work with cloud platforms (AWS, GCP, Azure) for scalable model training and deployment. âœ”ï¸ Deploy AI models using FastAPI, Flask, Docker, and Kubernetes in production environments. âœ”ï¸ Collaborate with cross-functional teams to build AI-powered applications and solutions. âœ”ï¸ Stay updated with cutting-edge advancements in AI, Deep Learning, and Generative AI.  Requirements: ğŸ“ Bachelorâ€™s/Masterâ€™s degree in Computer Science, AI, Data Science, or related fields. ğŸ’» 2-3 years of hands-on experience in Deep Learning, NLP, or Generative AI projects. ğŸ Strong Python skills (NumPy, Pandas, PyTorch, TensorFlow, Hugging Face Transformers). ğŸ§  Solid understanding of Neural Networks, Attention Mechanisms, and Transfer Learning. ğŸ” Experience with LLMs, RAG, and model fine-tuning using Hugging Face or OpenAI APIs. ğŸ“Š Proficiency in vector databases (FAISS, ChromaDB, Pinecone) for embedding search. â˜ï¸ Hands-on experience with Cloud AI services (AWS SageMaker, GCP Vertex AI, Azure AI). ğŸ› ï¸ Experience with MLOps tools (MLflow, Kubeflow, Docker, Kubernetes, CI/CD pipelines). ğŸ“ˆ Strong grasp of Deep Learning frameworks (PyTorch Lightning, TensorFlow/Keras). ğŸ’¡ Problem-solving mindset and ability to work on real-world AI challenges.  Nice to Have: âœ… Experience with Diffusion Models (Stable Diffusion, DALLÂ·E, Midjourney). âœ… Knowledge of Graph Neural Networks (GNNs) and Reinforcement Learning (RLHF). âœ… Exposure to AI Ethics, Bias Mitigation, and Explainable AI (XAI). âœ… Contributions to open-source AI projects or research publications.  ğŸš€ Join us and work on cutting-edge AI, Deep Learning, and Generative AI models!\n"},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\nAI Response:\n","output_type":"stream"},{"name":"stderr","text":"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-bca7f739cc90>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mpdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_pdf_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mjob_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Paste the job description: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mphi_3_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdf_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_description\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-bca7f739cc90>\u001b[0m in \u001b[0;36mphi_3_stream\u001b[0;34m(resume_text, job_description)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAI Response:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgeneration_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2224\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3202\u001b[0m         ):\n\u001b[1;32m   3203\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3204\u001b[0;31m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m             \u001b[0;31m# prepare variable output controls (note: some models won't accept all output controls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca/modeling_phi3.py\u001b[0m in \u001b[0;36mprepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0mcache_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seq_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                 \u001b[0mpast_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mmax_cache_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                 \u001b[0mcache_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n","\u001b[0;31mAttributeError\u001b[0m: 'DynamicCache' object has no attribute 'get_max_length'"],"ename":"AttributeError","evalue":"'DynamicCache' object has no attribute 'get_max_length'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# /kaggle/input/my-resume/N.surya_narayanan_resume_MscDs_.pdf (1) (1).pdf\nJob Title: Data Scientist (Deep Learning & Generative AI)\nğŸ“ Location: Chennai, Noida\nğŸ“„ Job Type: Full-time\nğŸ’¼ Experience: 2-3 years\n\nAbout the Role:\nWe are seeking a Data Scientist specializing in Deep Learning and Generative AI with 2-3 years of experience to join our AI-driven team. This role is ideal for someone passionate about Neural Networks, Transformers, Large Language Models (LLMs), and NLP. You will be responsible for building, fine-tuning, and deploying AI models using Hugging Face, TensorFlow, PyTorch, and OpenAI APIs.\n\nResponsibilities:\nâœ”ï¸ Develop and fine-tune Deep Learning models for NLP, CV, and Generative AI tasks.\nâœ”ï¸ Implement and optimize Transformer-based models (e.g., GPT, BERT, T5, LLaMA, Mistral).\nâœ”ï¸ Work with Hugging Faceâ€™s Transformers, Tokenizers, and Datasets for model training and deployment.\nâœ”ï¸ Design LLM-powered applications using LangChain, RAG, and Prompt Engineering techniques.\nâœ”ï¸ Handle data preprocessing, augmentation, and embedding techniques (FAISS, ChromaDB, Pinecone, Weaviate).\nâœ”ï¸ Train and optimize Diffusion Models (Stable Diffusion, DALLÂ·E) and GANs for image generation tasks.\nâœ”ï¸ Work with cloud platforms (AWS, GCP, Azure) for scalable model training and deployment.\nâœ”ï¸ Deploy AI models using FastAPI, Flask, Docker, and Kubernetes in production environments.\nâœ”ï¸ Collaborate with cross-functional teams to build AI-powered applications and solutions.\nâœ”ï¸ Stay updated with cutting-edge advancements in AI, Deep Learning, and Generative AI.\n\nRequirements:\nğŸ“ Bachelorâ€™s/Masterâ€™s degree in Computer Science, AI, Data Science, or related fields.\nğŸ’» 2-3 years of hands-on experience in Deep Learning, NLP, or Generative AI projects.\nğŸ Strong Python skills (NumPy, Pandas, PyTorch, TensorFlow, Hugging Face Transformers).\nğŸ§  Solid understanding of Neural Networks, Attention Mechanisms, and Transfer Learning.\nğŸ” Experience with LLMs, RAG, and model fine-tuning using Hugging Face or OpenAI APIs.\nğŸ“Š Proficiency in vector databases (FAISS, ChromaDB, Pinecone) for embedding search.\nâ˜ï¸ Hands-on experience with Cloud AI services (AWS SageMaker, GCP Vertex AI, Azure AI).\nğŸ› ï¸ Experience with MLOps tools (MLflow, Kubeflow, Docker, Kubernetes, CI/CD pipelines).\nğŸ“ˆ Strong grasp of Deep Learning frameworks (PyTorch Lightning, TensorFlow/Keras).\nğŸ’¡ Problem-solving mindset and ability to work on real-world AI challenges.\n\nNice to Have:\nâœ… Experience with Diffusion Models (Stable Diffusion, DALLÂ·E, Midjourney).\nâœ… Knowledge of Graph Neural Networks (GNNs) and Reinforcement Learning (RLHF).\nâœ… Exposure to AI Ethics, Bias Mitigation, and Explainable AI (XAI).\nâœ… Contributions to open-source AI projects or research publications.\n\nğŸš€ Join us and work on cutting-edge AI, Deep Learning, and Generative AI models!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ob Title: Data Science \n\nLocation: chennai,noida\nJob Type: Full-time\nExperience: 2-3 year\n\nAbout the Role:\nWe are looking for a highly motivated Data Science Fresher to join our team. This role is ideal for someone passionate about data analysis, machine learning, and AI-driven solutions. You will work with large datasets, develop predictive models, and gain hands-on experience with cutting-edge tools and technologies.\n\nResponsibilities:\nCollect, clean, and preprocess structured and unstructured data.\nPerform exploratory data analysis (EDA) to uncover insights.\nDevelop and implement machine learning models for predictive analytics.\nWork with tools like Python, SQL, and cloud platforms for data processing.\nCollaborate with cross-functional teams to solve real-world business problems.\nCreate dashboards and reports using visualization tools (Tableau, Power BI, Matplotlib, etc.).\nStay updated with the latest trends and advancements in data science.\nRequirements:\nBachelorâ€™s/Masterâ€™s degree in Computer Science, Data Science, Mathematics, or a related field.\nStrong programming skills in Python (NumPy, Pandas, Scikit-Learn) or R.\nKnowledge of SQL for data extraction and manipulation.\nUnderstanding of machine learning concepts and algorithms.\nFamiliarity with data visualization tools like Matplotlib, Seaborn, or Power BI.\nBasic knowledge of cloud platforms (AWS, Azure, or GCP) is a plus.\nStrong problem-solving skills and attention to detail.\nGood communication skills and a willingness to learn.\nNice to Have:\nExperience with NLP, Deep Learning, or Big Data technologies.\nHands-on projects or internships in data science.\nFamiliarity with tools like Git, Docker, or MLOps frameworks.","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}