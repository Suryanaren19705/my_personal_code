{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":913088,"sourceType":"datasetVersion","datasetId":490532}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**AIM**\n\nCode to Create a Finetuned Model for Question-Answer Model","metadata":{}},{"cell_type":"markdown","source":"# BERT-BASE-UNCASED\n\nBERT Base Uncased is a pretrained transformer model that processes text bidirectionally without case sensitivity. It is trained on Wikipedia and BooksCorpus using Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). It is used for text classification, question answering, NER, and semantic search in NLP applications","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tuning\n\nFine-tuning is the process of taking a pretrained model and training it further on a specific dataset to adapt it for a particular task. Instead of training from scratch, fine-tuning modifies the model's weights slightly, making it more specialized while retaining its general knowledge. This is widely used in LLMs, NLP, and computer vision to improve task-specific performance with minimal resources","metadata":{}},{"cell_type":"markdown","source":"# LoRA\n\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that freezes the original model weights and introduces small, trainable low-rank matrices into transformer layers. This significantly reduces memory usage and computational cost while achieving similar performance to full fine-tuning","metadata":{}},{"cell_type":"markdown","source":"Install the Packages","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:58:50.585878Z","iopub.execute_input":"2025-03-13T12:58:50.586273Z","iopub.status.idle":"2025-03-13T12:59:01.384340Z","shell.execute_reply.started":"2025-03-13T12:58:50.586240Z","shell.execute_reply":"2025-03-13T12:59:01.383416Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import the Required Packages ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\nfrom datasets import Dataset\nfrom transformers import AutoModelForQuestionAnswering,AutoTokenizer,Trainer,TrainingArguments,PreTrainedTokenizer\nimport torch\nfrom peft import LoraConfig,get_peft_model,TaskType","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:01.386234Z","iopub.execute_input":"2025-03-13T12:59:01.386526Z","iopub.status.idle":"2025-03-13T12:59:22.053735Z","shell.execute_reply.started":"2025-03-13T12:59:01.386498Z","shell.execute_reply":"2025-03-13T12:59:22.053004Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the `Dataset` as json format","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/quac-question-answering-in-context-dataset/train_v0.2 QuaC.json', 'r') as f:\n    data = json.load(f)\n    print(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:22.054849Z","iopub.execute_input":"2025-03-13T12:59:22.055585Z","iopub.status.idle":"2025-03-13T12:59:25.971978Z","shell.execute_reply.started":"2025-03-13T12:59:22.055544Z","shell.execute_reply":"2025-03-13T12:59:25.970326Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fetch the `Context` , `Question` & `Answer` from the json and store it in the list","metadata":{}},{"cell_type":"code","source":"cont=[]\nque=[]\nans=[]\nfor i in range(len(data['data'])):\n    con=data['data'][i]['paragraphs'][0]['context']\n    question=data['data'][i]['paragraphs'][0]['qas'][0]['question']\n    answer=data['data'][i]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n    cont.append(con)\n    que.append(question)\n    ans.append(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:25.975341Z","iopub.execute_input":"2025-03-13T12:59:25.975877Z","iopub.status.idle":"2025-03-13T12:59:26.027798Z","shell.execute_reply.started":"2025-03-13T12:59:25.975816Z","shell.execute_reply":"2025-03-13T12:59:26.026075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Store the List into a Dataframe","metadata":{}},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['context']=cont\ndf['question']=que\ndf['answer']=ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:26.029052Z","iopub.execute_input":"2025-03-13T12:59:26.029315Z","iopub.status.idle":"2025-03-13T12:59:26.062290Z","shell.execute_reply.started":"2025-03-13T12:59:26.029292Z","shell.execute_reply":"2025-03-13T12:59:26.060739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:26.063866Z","iopub.execute_input":"2025-03-13T12:59:26.064429Z","iopub.status.idle":"2025-03-13T12:59:26.095770Z","shell.execute_reply.started":"2025-03-13T12:59:26.064360Z","shell.execute_reply":"2025-03-13T12:59:26.094770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the bert-base-uncased model for Context based Question Answer model Finetuning with LoRA","metadata":{}},{"cell_type":"code","source":"model_nm='bert-base-uncased'\nmodel=AutoModelForQuestionAnswering.from_pretrained(model_nm)\ntok=AutoTokenizer.from_pretrained(model_nm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:26.096990Z","iopub.execute_input":"2025-03-13T12:59:26.097297Z","iopub.status.idle":"2025-03-13T12:59:29.420419Z","shell.execute_reply.started":"2025-03-13T12:59:26.097258Z","shell.execute_reply":"2025-03-13T12:59:29.419360Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the `Maximum Length` and Some `Function` For the Tokenization Process ","metadata":{}},{"cell_type":"code","source":"max_length = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:29.421947Z","iopub.execute_input":"2025-03-13T12:59:29.422890Z","iopub.status.idle":"2025-03-13T12:59:29.426802Z","shell.execute_reply.started":"2025-03-13T12:59:29.422843Z","shell.execute_reply":"2025-03-13T12:59:29.425847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preprocessing steps :\n\n1) preprocess the `Context` and `Question` and take all the inputs from that\n2) Preprocess the Answer seperatly and take only the `input_ids` from that output\n3) Define the `Start Position`&`End position` in the given contex based on the answer.","metadata":{}},{"cell_type":"markdown","source":"1) `get_answer_positions` this function is used to get the start & end position of the answer in the given contect\n2) `toke_batch` this function is used to get the preprocessed output of the `Context` & `Question`\n3) `lab_toke_batch` this function is used to take the preprocessed output of the answer & combined the output of all the inputs to make sure the dataset ready for model training","metadata":{}},{"cell_type":"code","source":"tok.add_special_tokens({'pad_token': '[PAD]'})\ndef get_answer_positions(context, answer, tokenizer):\n    \"\"\"\n    Find the start and end token positions of the answer in the context.\n    \"\"\"\n    # Encode the context and answer to find the token positions\n    start_idx = context.find(answer)\n    end_idx = start_idx + len(answer)\n    \n    # Tokenize the context\n    tokenized_context = tokenizer(context, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n    \n    # Find the start and end positions of the answer in tokenized form\n    start_token = tokenizer.encode(context[:start_idx], add_special_tokens=False)\n    end_token = tokenizer.encode(context[:end_idx], add_special_tokens=False)\n    \n    start_pos = len(start_token)  # start position in tokenized input\n    end_pos = len(end_token) - 1  # end position in tokenized input\n    \n    return start_pos, end_pos\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:29.427768Z","iopub.execute_input":"2025-03-13T12:59:29.428019Z","iopub.status.idle":"2025-03-13T12:59:29.434388Z","shell.execute_reply.started":"2025-03-13T12:59:29.427996Z","shell.execute_reply":"2025-03-13T12:59:29.433490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def toke_batch(examples, tokenizer: PreTrainedTokenizer):\n    \"\"\"\n    Tokenizes the input data (context + question) into tensors for model input.\n    \"\"\"\n    # Create input prompts: Context + Question\n    input_prompts = [f\"Context: {example['context']} Question: {example['question']} Answer:\" for example in examples]\n    \n    # Tokenize the input prompts with truncation and padding to max_length\n    input_encodings = tokenizer(input_prompts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n    \n    return input_encodings\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:29.436716Z","iopub.execute_input":"2025-03-13T12:59:29.436980Z","iopub.status.idle":"2025-03-13T12:59:29.441433Z","shell.execute_reply.started":"2025-03-13T12:59:29.436956Z","shell.execute_reply":"2025-03-13T12:59:29.440605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def lab_toke_batch(examples, tokenizer: PreTrainedTokenizer):\n    \"\"\"\n    Tokenizes the labels (answers) into tensors for model labels.\n    \"\"\"\n    # Extract all answers and calculate the start and end positions for each answer\n    start_positions = []\n    end_positions = []\n    labels = []\n    \n    for example in examples:\n        context = example['context']\n        answer = example['answer']\n        \n        # Get start and end positions\n        start_pos, end_pos = get_answer_positions(context, answer, tokenizer)\n        \n        # Append start and end positions for each example\n        start_positions.append(start_pos)\n        end_positions.append(end_pos)\n        \n        # Tokenize the answer (label)\n        labels.append(answer)\n    \n    # Tokenize the labels (answers)\n    label_encodings = tokenizer(labels, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n    \n    return label_encodings, start_positions, end_positions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:29.442532Z","iopub.execute_input":"2025-03-13T12:59:29.442823Z","iopub.status.idle":"2025-03-13T12:59:29.447694Z","shell.execute_reply.started":"2025-03-13T12:59:29.442796Z","shell.execute_reply":"2025-03-13T12:59:29.446957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Code to do call the function and ready the dataset for model finetuning","metadata":{}},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(df)\ntest_dataset = Dataset.from_pandas(df)\n\n# Apply batch tokenization to the training dataset\ntrain_inputs = toke_batch(train_dataset, tok)\ntrain_labels, train_start_positions, train_end_positions = lab_toke_batch(train_dataset, tok)\n\n# Apply batch tokenization to the test dataset\ntest_inputs = toke_batch(test_dataset, tok)\ntest_labels, test_start_positions, test_end_positions = lab_toke_batch(test_dataset, tok)\n\n# Add 'labels', 'start_positions' and 'end_positions' to input data\ntrain_inputs['labels'] = train_labels.input_ids\ntrain_inputs['start_positions'] = torch.tensor(train_start_positions)\ntrain_inputs['end_positions'] = torch.tensor(train_end_positions)\n\ntest_inputs['labels'] = test_labels.input_ids\ntest_inputs['start_positions'] = torch.tensor(test_start_positions)\ntest_inputs['end_positions'] = torch.tensor(test_end_positions)\n\n# Convert to Hugging Face Dataset format\ntraining_dataset = Dataset.from_dict(train_inputs)\ntesting_dataset = Dataset.from_dict(test_inputs)\n\n# Now you have a dataset with 'start_positions' and 'end_positions' for each example\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:59:29.449006Z","iopub.execute_input":"2025-03-13T12:59:29.449331Z","iopub.status.idle":"2025-03-13T13:01:05.735581Z","shell.execute_reply.started":"2025-03-13T12:59:29.449296Z","shell.execute_reply":"2025-03-13T13:01:05.734867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Output of the training dataset","metadata":{}},{"cell_type":"code","source":"training_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:01:05.736584Z","iopub.execute_input":"2025-03-13T13:01:05.736855Z","iopub.status.idle":"2025-03-13T13:01:05.742115Z","shell.execute_reply.started":"2025-03-13T13:01:05.736831Z","shell.execute_reply":"2025-03-13T13:01:05.741232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the LoRA Config For The Model Finetuning with some parameterts","metadata":{}},{"cell_type":"code","source":"con=LoraConfig(r=8,lora_alpha=32,lora_dropout=0.1,task_type=TaskType.QUESTION_ANS)\npef_mod=get_peft_model(model,con)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:01:05.743128Z","iopub.execute_input":"2025-03-13T13:01:05.743396Z","iopub.status.idle":"2025-03-13T13:01:05.787924Z","shell.execute_reply.started":"2025-03-13T13:01:05.743364Z","shell.execute_reply":"2025-03-13T13:01:05.786984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Initiate the Training Arguments For Model finetuning and Train the Model\n\nThe Finetuned model File is saved in **`/kaggle/working/`** this path","metadata":{}},{"cell_type":"code","source":"tr_args = TrainingArguments(\n    output_dir='/kaggle/working/',\n    evaluation_strategy='epoch',\n    num_train_epochs=1,\n    eval_steps=1,\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n      # Force training on CPU\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:01:05.789074Z","iopub.execute_input":"2025-03-13T13:01:05.789700Z","iopub.status.idle":"2025-03-13T13:01:05.915566Z","shell.execute_reply.started":"2025-03-13T13:01:05.789652Z","shell.execute_reply":"2025-03-13T13:01:05.914669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    args=tr_args,\n    model=pef_mod,\n    train_dataset=training_dataset,\n    eval_dataset=testing_dataset\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:01:05.916825Z","iopub.execute_input":"2025-03-13T13:01:05.917194Z","iopub.status.idle":"2025-03-13T13:01:07.641351Z","shell.execute_reply.started":"2025-03-13T13:01:05.917154Z","shell.execute_reply":"2025-03-13T13:01:07.640561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the Model with 1 epochs and it will evaluate the output for each steps","metadata":{}},{"cell_type":"code","source":"\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:01:07.642171Z","iopub.execute_input":"2025-03-13T13:01:07.642449Z","iopub.status.idle":"2025-03-13T13:19:16.985394Z","shell.execute_reply.started":"2025-03-13T13:01:07.642400Z","shell.execute_reply":"2025-03-13T13:19:16.984435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking the output Mannualy","metadata":{}},{"cell_type":"code","source":"i=1\ncon=df['context'][i]\nque=df['question'][i]\nans=df['answer'][i]\nprint(que,ans) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:22:29.346313Z","iopub.execute_input":"2025-03-13T13:22:29.347101Z","iopub.status.idle":"2025-03-13T13:22:29.354035Z","shell.execute_reply.started":"2025-03-13T13:22:29.347064Z","shell.execute_reply":"2025-03-13T13:22:29.352981Z"}},"outputs":[{"name":"stdout","text":"what language do they speak? Malayalam is the language spoken by the Malayalis.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"Answer For the Question which asked to the Model after Finetunined","metadata":{}},{"cell_type":"code","source":"# Load the trained model and tokenizer (replace with actual path if needed)\ntokenizer = tok  # Ensure 'tok' is the tokenizer object\nmodel = pef_mod   # Ensure 'pef_mod' is the fine-tuned model object\n\n# Move model to the correct device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define a manual question and context\ncontext = con\nquestion = que\n\n# Tokenize the inputs\ninputs = tokenizer(\n    question,\n    context,\n    return_tensors=\"pt\",\n    truncation=True,\n    max_length=512,\n    padding=\"max_length\",\n)\n\n# Move inputs to the correct device\ninputs = {key: value.to(device) for key, value in inputs.items()}\n\n# Get model predictions\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extract start and end logits\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\n\n# Get the most probable start and end positions\nstart_index = torch.argmax(start_logits)\nend_index = torch.argmax(end_logits)\n\n# Decode the answer\nanswer = tokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index + 1], skip_special_tokens=True)\n\n# Print the results\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")\nprint(f\"ActualAns:{ans}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:22:30.863907Z","iopub.execute_input":"2025-03-13T13:22:30.864511Z","iopub.status.idle":"2025-03-13T13:22:30.903741Z","shell.execute_reply.started":"2025-03-13T13:22:30.864477Z","shell.execute_reply":"2025-03-13T13:22:30.902835Z"}},"outputs":[{"name":"stdout","text":"Question: what language do they speak?\nAnswer: what language do they speak? malayalam is the language spoken by the malayalis. malayalam is derived from old tamil and sanskrit in the\nActualAns:Malayalam is the language spoken by the Malayalis.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}